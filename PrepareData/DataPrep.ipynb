{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMI_dir = os.getcwd()\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def save_file(text, direc, filename):\n",
    "    ensure_dir(direc)\n",
    "    file = open(direc + filename, 'w')\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    \n",
    "ami_folder = AMI_dir + '\\\\ami_public_manual_1.6.2\\\\'\n",
    "dialog_acts = AMI_dir + '\\\\DialogActFiles\\\\'\n",
    "ensure_dir(dialog_acts)\n",
    "decisions = AMI_dir + '\\\\DecisionFiles\\\\'\n",
    "ensure_dir(decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all the sentences along with their corresponding Dialogue Acts(DA's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each meeting transcript will be stored with each line representing a Dialogue Act. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of each line will be: Speaker    Start Time    Sentence    DA_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    '''\n",
    "    This method reads through all the files in the AMI corpus and \n",
    "    saves the dialogue acts for each meeting as a text file for further processing.\n",
    "    '''\n",
    "    dialogs = {}  # store dialogs from the corpus\n",
    "    dialog_names = []  # store file names from the corpus\n",
    "    csv_corpus = []\n",
    "    \n",
    "    # Get every file\n",
    "    dialogueAct_files = [f for f in os.listdir(ami_folder + \"dialogueActs\\\\\") if f.endswith('act.xml')]\n",
    "    \n",
    "    # Group speaker files by meeting\n",
    "    group_speaker_files = defaultdict(list)\n",
    "\n",
    "    for t in dialogueAct_files:\n",
    "        meeting = t.split('.')[0]\n",
    "        if bool(group_speaker_files[meeting]):\n",
    "            group_speaker_files[meeting].append(t.split('dialog')[0])\n",
    "        else:\n",
    "            group_speaker_files[meeting] = [t.split('dialog')[0]]\n",
    "    \n",
    "    # Group transcripts by meeting\n",
    "    for key in group_speaker_files.keys():\n",
    "        da_speakers_filename = group_speaker_files[key]\n",
    "        for da_filename in da_speakers_filename:    \n",
    "            dialogs[da_filename] = OrderedDict()\n",
    "            load_words(dialogs, da_filename)\n",
    "            load_dialog_acts(dialogs, da_filename)\n",
    "            \n",
    "        csv_corpus = create_csv(dialogs)\n",
    "\n",
    "        with open(dialog_acts + key + '.dialog-act.txt', mode='w+') as file:\n",
    "            mainlist = sorted(csv_corpus, key=lambda n: (float(n[1])))\n",
    "            for sublist in mainlist:\n",
    "                file.write(' '.join(sublist) + '\\n')\n",
    "        dialogs.clear()\n",
    "\n",
    "    print(\"Number of DA annotated meetings: {}\".format(len(group_speaker_files.keys())))\n",
    "        \n",
    "def load_words(dialogs, dialog_name):\n",
    "    '''\n",
    "    This method reads all the meeting transcript which are stored word by word. \n",
    "    Also, cleans the word by removing any disfluencies and expanding any contractions.\n",
    "    '''\n",
    "    \n",
    "    flag = True\n",
    "    \n",
    "    disfluencies = [\"um\", \"uhm\", \"mm\", \"mmm\", \"ehm\", \"mmh\", \"uhmm\", \"ah\", \"hmm\", \"hm-hmm\", \"mm-hmm\", \"uh\", \"blah\", \"bah\", \"ye\", \n",
    "                    \"Um\", \"Uhm\", \"Mm\", \"Mmm\", \"Ehm\", \"Mmh\", \"Uhmm\", \"Ah\", \"Hmm\", \"Hm-hmm\", \"Mm-hmm\", \"Uh\", \"Blah\", \"Bah\", \"Ye\",]\n",
    "    \n",
    "    with open(ami_folder + \"words\\\\\" + dialog_name + \"words.xml\") as wfile:\n",
    "        for line in wfile:\n",
    "            if \"<w\" not in line:  # not a word\n",
    "                continue\n",
    "            \n",
    "            if 'starttime' in line:\n",
    "                word_id = line.split(\"id=\\\"\")[1].split(\"\\\"\")[0].split(\".\")[-1]\n",
    "                word_value = line.split(\">\")[1].split(\"<\")[0]\n",
    "                word_value = word_value.strip().replace(\"&#39;\", \"'\")\n",
    "                speaker = line.split(\"id=\\\"\")[1].split('.')[1]\n",
    "                \n",
    "                if flag:\n",
    "                    starttime = line.split(\"starttime=\\\"\")[1].split(\"\\\"\")[0]\n",
    "                    flag = False\n",
    "                \n",
    "                if word_value not in disfluencies:\n",
    "                    word_value = word_value.replace(\"'m\",\" am\")\n",
    "                    word_value = word_value.replace(\"'ve\",\" have\")\n",
    "                    word_value = word_value.replace(\"'s\",\" is\")\n",
    "                    word_value = word_value.replace(\"'S\",\"Is\")\n",
    "                    word_value = word_value.replace(\"'re\",\" are\")\n",
    "                    word_value = word_value.replace(\"'ll\",\" will\")\n",
    "                    word_value = word_value.replace(\"'d\",\" would\")\n",
    "                    word_value = word_value.replace(\"n't\",\" not\")\n",
    "                    word_value = word_value.replace(\"n'\",\" not\")\n",
    "                    word_value = word_value.replace(\"'em\",\"them\")\n",
    "                    word_value = word_value.replace(\"an'\",\"and\")\n",
    "                    word_value = word_value.replace(\"'tis\",\"it is\")\n",
    "                    word_value = word_value.replace(\"it'\",\"it is\")\n",
    "                    word_value = word_value.replace(\"'Kay\",\"Okay\")\n",
    "                    word_value = word_value.replace(\"'kay\",\"okay\")\n",
    "                    word_value = word_value.replace(\"'cause\",\"because\")\n",
    "                    word_value = word_value.replace(\"'Cause\",\"Because\")\n",
    "                    word_value = word_value.replace(\"'cau\",\"cause\")\n",
    "                    word_value = word_value.replace(\"'Cau\",\"Cause\")\n",
    "                    word_value = word_value.replace(\"'til\",\"until\")\n",
    "                    word_value = word_value.replace(\"y'all\",\"you all\")\n",
    "                    word_value = word_value.replace(\"ya'\",\"you\")\n",
    "                    word_value = word_value.replace(\"'bout\",\"about\")\n",
    "                    word_value = word_value.replace(\"'Bout\",\"About\")\n",
    "                    word_value = word_value.replace(\"'Fraid\",\"Afraid\")\n",
    "                    word_value = word_value.replace(\"'fraid\",\"afraid\")\n",
    "                    word_value = word_value.replace(\"'round\",\"around\")\n",
    "                    word_value = word_value.replace(\"gonna\",\"going to\")\n",
    "                    word_value = word_value.replace(\"wanna\",\"want to\")\n",
    "                    word_value = word_value.replace(\"kinda\",\"kind of\")\n",
    "                    word_value = word_value.replace(\"_\",\"\")\n",
    "                    word_value = word_value.replace(\"..\",\".\")\n",
    "                    \n",
    "                    if len(word_value) > 3:\n",
    "                        word_value = word_value.replace(\"-\",\" \")\n",
    "                    else:\n",
    "                        word_value = word_value.replace(\"-\",\"\")\n",
    "                    \n",
    "                    dialogs[dialog_name][word_id] = []\n",
    "                    dialogs[dialog_name][word_id].append(speaker)\n",
    "                    dialogs[dialog_name][word_id].append(starttime)\n",
    "                    dialogs[dialog_name][word_id].append(word_value)\n",
    "                \n",
    "            if word_value == '.':\n",
    "                flag = True\n",
    "                                                 \n",
    "def load_dialog_acts(dialogs, dialog_name):\n",
    "    '''\n",
    "    This method combines all the words to form a sentence based on the annotation for dialogue acts as per AMI corpus.\n",
    "    '''\n",
    "    \n",
    "    with open(ami_folder + \"dialogueActs\\\\\" + dialog_name + \"dialog-act.xml\") as actfile:\n",
    "        dact = \"\"\n",
    "        for line in actfile:\n",
    "            if \"<nite:pointer\" in line:  # act definition\n",
    "                dact = line.split(\"href=\\\"da-types.xml#id(\")[1].split(\")\")[0]\n",
    "                continue\n",
    "            elif \"<nite:child\" in line:  # word list for this act\n",
    "                ids = line.split(\"#\")[1]\n",
    "                # 4.1 get the start/stopIDs to be queried\n",
    "                start_id = ids.split(\"..\")[0].split(\"(\")[1].split(\")\")[0].split(\"words\")[1]\n",
    "                # 4.2 Get the range of IDs to be queried\n",
    "                try:\n",
    "                    stop_id = ids.split(\"..\")[1].split(\"(\")[1].split(\")\")[0].split(\"words\")[1]\n",
    "                except:\n",
    "                    stop_id = start_id \n",
    "                start_n = int(start_id)\n",
    "                stop_n = int(stop_id)\n",
    "                # 4. Build the query\n",
    "                set = [\"words\" + str(i) for i in range(start_n, stop_n + 1)]\n",
    "                for w in set:\n",
    "                    try:\n",
    "                        dialogs[dialog_name][w].append(dact)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "    \n",
    "def create_csv(dialogs):\n",
    "    '''\n",
    "    This method acts as a temporary stage to store all the dialogue acts for a particulat meeting.\n",
    "    '''\n",
    "    csv_corpus = []\n",
    "    for d in dialogs:\n",
    "        sentence = \"\"\n",
    "        prevDA = \"Other\"\n",
    "        currentDA = \"\"\n",
    "        sentStartTime = 0\n",
    "        endOfSent = ''\n",
    "        currSpeaker = ''\n",
    "        for word in dialogs[d]:\n",
    "            try:\n",
    "                speaker, starttime, word, DA = dialogs[d][word]\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if (DA != currentDA or speaker != currSpeaker or endOfSent == '.') and sentence != \"\":# new DA or speaker or EOL\n",
    "                doc = nlp(sentence.strip())\n",
    "                sentences = list(doc.sents)\n",
    "                for sent in sentences:      \n",
    "                    if re.search('[a-zA-Z]+', str(sent)):\n",
    "                        csv_corpus.append((speaker, sentStartTime, str(sent), currentDA))\n",
    "                sentence = \"\"\n",
    "                prevDA = currentDA\n",
    "            endOfSent = word\n",
    "            sentStartTime = starttime\n",
    "            currentDA = DA\n",
    "            currSpeaker = speaker\n",
    "            sentence = sentence + \" \" + (word.strip())\n",
    "    return csv_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of DA annotated meetings: 139\n"
     ]
    }
   ],
   "source": [
    "load_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all decision transcripts in format: Speaker    Start_Time    Word    Decision (0 - False / 1 - True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_decisions():\n",
    "    \n",
    "    # Get every file\n",
    "    decision_files = [f for f in os.listdir(ami_folder + \"decision\\\\manual\\\\\")]\n",
    "    decisions_words = []\n",
    "    load_decisions_words(decision_files, decisions_words)\n",
    "    decisions_words.sort()\n",
    "    \n",
    "    for files in decision_files:\n",
    "        words = []\n",
    "        filename = files.split('.')[0]\n",
    "        words_files = [f for f in os.listdir(ami_folder + \"words\\\\\") if f.startswith(filename)]\n",
    "        for file in words_files:\n",
    "            load_words(file, words, decisions_words)\n",
    "#         print(words)\n",
    "        save_dec_file(decisions, filename, words)\n",
    "    \n",
    "    print(\"Number of Decision annotated meetings: {}\".format(len(decision_files)))\n",
    "\n",
    "def save_dec_file(decisions, filename, words):\n",
    "#     print('Saving file {}.decision'.format(filename))\n",
    "    with open(decisions + filename + '.decision.txt', mode='w+') as file:\n",
    "        mainlist = sorted(words, key=lambda n: (float(n[1])))\n",
    "        for sublist in mainlist:\n",
    "            file.write(' '.join(sublist) + '\\n') \n",
    "        \n",
    "def load_words(file, words, decisions_words):\n",
    "    \n",
    "    disfluencies = [\"um\", \"uhm\", \"mm\", \"mmm\", \"ehm\", \"mmh\", \"uhmm\", \"ah\", \"hmm\", \"hm-hmm\", \"mm-hmm\", \"uh\", \"blah\", \"bah\", \"ye\", \n",
    "                    \"Um\", \"Uhm\", \"Mm\", \"Mmm\", \"Ehm\", \"Mmh\", \"Uhmm\", \"Ah\", \"Hmm\", \"Hm-hmm\", \"Mm-hmm\", \"Uh\", \"Blah\", \"Bah\", \"Ye\",]\n",
    "    \n",
    "    with open(ami_folder + \"words\\\\\" + file) as wfile:\n",
    "        for line in wfile:\n",
    "            if \"<w\" not in line:  # not a word\n",
    "                continue\n",
    "            if 'starttime' in line:\n",
    "                key = line.split(\"id=\\\"\")[1].split(\"\\\"\")[0]\n",
    "                word_value = line.split(\">\")[1].split(\"<\")[0]\n",
    "                word_value = word_value.strip().replace(\"&#39;\", \"'\")\n",
    "                starttime = line.split(\"starttime=\\\"\")[1].split(\"\\\"\")[0]\n",
    "                speaker = line.split(\"id=\\\"\")[1].split('.')[1]\n",
    "                \n",
    "                if word_value not in disfluencies:\n",
    "                    word_value = word_value.replace(\"'m\",\" am\")\n",
    "                    word_value = word_value.replace(\"'ve\",\" have\")\n",
    "                    word_value = word_value.replace(\"'s\",\" is\")\n",
    "                    word_value = word_value.replace(\"'S\",\"Is\")\n",
    "                    word_value = word_value.replace(\"'re\",\" are\")\n",
    "                    word_value = word_value.replace(\"'ll\",\" will\")\n",
    "                    word_value = word_value.replace(\"'d\",\" would\")\n",
    "                    word_value = word_value.replace(\"n't\",\" not\")\n",
    "                    word_value = word_value.replace(\"n'\",\" not\")\n",
    "                    word_value = word_value.replace(\"'em\",\"them\")\n",
    "                    word_value = word_value.replace(\"an'\",\"and\")\n",
    "                    word_value = word_value.replace(\"'tis\",\"it is\")\n",
    "                    word_value = word_value.replace(\"it'\",\"it is\")\n",
    "                    word_value = word_value.replace(\"'Kay\",\"Okay\")\n",
    "                    word_value = word_value.replace(\"'kay\",\"okay\")\n",
    "                    word_value = word_value.replace(\"'cause\",\"because\")\n",
    "                    word_value = word_value.replace(\"'Cause\",\"Because\")\n",
    "                    word_value = word_value.replace(\"'cau\",\"cause\")\n",
    "                    word_value = word_value.replace(\"'Cau\",\"Cause\")\n",
    "                    word_value = word_value.replace(\"'til\",\"until\")\n",
    "                    word_value = word_value.replace(\"y'all\",\"you all\")\n",
    "                    word_value = word_value.replace(\"ya'\",\"you\")\n",
    "                    word_value = word_value.replace(\"'bout\",\"about\")\n",
    "                    word_value = word_value.replace(\"'Bout\",\"About\")\n",
    "                    word_value = word_value.replace(\"'Fraid\",\"Afraid\")\n",
    "                    word_value = word_value.replace(\"'fraid\",\"afraid\")\n",
    "                    word_value = word_value.replace(\"'round\",\"around\")\n",
    "                    word_value = word_value.replace(\"gonna\",\"going to\")\n",
    "                    word_value = word_value.replace(\"wanna\",\"want to\")\n",
    "                    word_value = word_value.replace(\"kinda\",\"kind of\")\n",
    "                    word_value = word_value.replace(\"_\",\"\")\n",
    "                    word_value = word_value.replace(\"..\",\".\")\n",
    "                    \n",
    "                    if len(word_value) > 3:\n",
    "                        word_value = word_value.replace(\"-\",\" \")\n",
    "                    else:\n",
    "                        word_value = word_value.replace(\"-\",\"\")\n",
    "                    \n",
    "                    doc = nlp(word_value)\n",
    "                    for token in doc:\n",
    "                        if key in decisions_words:\n",
    "                            words.append((speaker, starttime, str(token), '1'))\n",
    "                        else:\n",
    "                            words.append((speaker, starttime, str(token), '0'))\n",
    "                                                      \n",
    "def load_decisions_words(decision_files, decisions_words):\n",
    "    \n",
    "    for decision_name in decision_files:\n",
    "        with open(ami_folder + \"decision\\\\manual\\\\\" + decision_name) as actfile:\n",
    "            for line in actfile:\n",
    "                if \"<nite:child\" in line:  # word list for this act\n",
    "                    file = line.split(\"href=\\\"\")[1].split(\"words.xml\")[0]\n",
    "                    ids = line.split(\"#\")[1]\n",
    "                    # 4.1 get the start/stopIDs to be queried\n",
    "                    start_id = ids.split(\"..\")[0].split(\"(\")[1].split(\")\")[0].split(\"words\")[1]\n",
    "                    # 4.2 Get the range of IDs to be queried\n",
    "                    try:\n",
    "                        stop_id = ids.split(\"..\")[1].split(\"(\")[1].split(\")\")[0].split(\"words\")[1]\n",
    "                    except:\n",
    "                        stop_id = start_id \n",
    "                    start_n = int(start_id)\n",
    "                    stop_n = int(stop_id)\n",
    "                    # 4. Build the query\n",
    "                    set = [\"words\" + str(i) for i in range(start_n, stop_n + 1)]\n",
    "                    \n",
    "                    for w in set:\n",
    "                        decisions_words.append(file + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Decision annotated meetings: 47\n"
     ]
    }
   ],
   "source": [
    "load_decisions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data files created above into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decTrain = AMI_dir + '\\\\Dec_Train'\n",
    "ensure_dir(decTrain + '\\\\')\n",
    "\n",
    "decTest = AMI_dir + '\\\\Dec_Test'\n",
    "ensure_dir(decTest + '\\\\')\n",
    "\n",
    "daTrain = AMI_dir + '\\\\DA_Train'\n",
    "ensure_dir(daTrain + '\\\\')\n",
    "\n",
    "daTest = AMI_dir + '\\\\DA_Test'\n",
    "ensure_dir(daTest + '\\\\')  \n",
    "\n",
    "srcDec = AMI_dir + '\\\\DecisionFiles'\n",
    "srcDa = AMI_dir + '\\\\DialogActFiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(srcDec)\n",
    "DecTrain = files[:40]\n",
    "DecTest = files[40:47]\n",
    "\n",
    "files = os.listdir(srcDa)\n",
    "DaTrain = files[:124]\n",
    "DaTest = files[124:139]\n",
    "\n",
    "for i in DecTrain:\n",
    "    shutil.copy(os.path.join(srcDec, i), decTrain)\n",
    "\n",
    "for i in DecTest:\n",
    "    shutil.copy(os.path.join(srcDec, i), decTest)\n",
    "\n",
    "for i in DaTrain:\n",
    "    shutil.copy(os.path.join(srcDa, i), daTrain)\n",
    "\n",
    "for i in DaTest:\n",
    "    shutil.copy(os.path.join(srcDa, i), daTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
